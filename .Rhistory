threshold = best_rmse * 1.01
for i, rmse in enumerate(rmse_values):
if rmse <= threshold:
print(f"Smallest Dimensionality within 1% of Best RMSE: {i}")
break
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree
data = pd.read_csv("mol.csv")
data_cleaned = data.drop(columns=["parentspecies", "log_pSat_Pa"])
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_cleaned)
data_non_scaled = data_cleaned.values
def kmeans_loss(data, max_clusters=20):
losses = []
for k in range(1, max_clusters + 1):
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(data)
losses.append(kmeans.inertia_)
return losses
losses_scaled = kmeans_loss(data_scaled)
losses_non_scaled = kmeans_loss(data_non_scaled)
# Plot the losses
plt.figure(figsize=(10, 6))
plt.plot(range(1, 21), losses_scaled, label="Scaled Data", marker='o')
plt.plot(range(1, 21), losses_non_scaled, label="Non-Scaled Data", marker='x')
plt.title("K-means Loss vs. Number of Clusters")
plt.xlabel("Number of Clusters")
plt.ylabel("K-means Loss (Inertia)")
plt.legend()
plt.grid(alpha=0.3)
plt.show()
k = 5
losses = []
for _ in range(1000):
kmeans = KMeans(n_clusters=k, init="random", n_init=1, random_state=None)
kmeans.fit(data_scaled)
losses.append(kmeans.inertia_)
min_loss = np.min(losses)
max_loss = np.max(losses)
threshold = min_loss * 1.01
good_initializations = np.sum(np.array(losses) <= threshold)
prob = good_initializations/len(losses)
needed = 1/prob
plt.figure(figsize=(10, 6))
plt.hist(losses, bins=100, alpha=0.7, color='blue')
plt.title("Distribution of K-means Losses (1000 Random Initializations)")
plt.xlabel("K-means Loss (Inertia)")
plt.ylabel("Frequency")
plt.grid(alpha=0.3)
plt.show()
print(f"Minimum loss: {min_loss}")
print(f"Maximum loss: {max_loss}")
print(f"Number of good initializations: {good_initializations}")
print(f"Initilizations needed: {needed}")
linkage_methods = ['single', 'complete', 'ward']
fig, axes = plt.subplots(1, 3, figsize=(18, 6))
for i, method in enumerate(linkage_methods):
Z = linkage(data_scaled, method=method)
dendrogram(Z, ax=axes[i], truncate_mode='level', p=10)
axes[i].set_title(f"{method}")
axes[i].set_xlabel("Samples")
axes[i].set_ylabel("Distance")
plt.tight_layout()
plt.show()
methods = ['single', 'complete', 'ward']
linkages = {method: linkage(data_scaled, method=method) for method in methods}
# Flat clusterings using cut_tree
n_clusters = 5  # Specify the number of clusters for comparison
cluster_labels = {method: cut_tree(linkages[method], n_clusters=n_clusters).flatten() for method in methods}
# Compare cluster sizes for different linkage methods
cluster_sizes = {
method: np.bincount(labels) for method, labels in cluster_labels.items()
}
# Display cluster sizes for each method
print("Cluster Sizes:")
for method, sizes in cluster_sizes.items():
print(f"{method.capitalize()} Linkage: {sizes}")
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# Load data
data = pd.read_csv('mol.csv')
# Separate features and target
features = data.drop(columns=['parentspecies', 'log_pSat_Pa'])
target = data['parentspecies']
# Scale the data
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
# Apply PCA
pca = PCA(n_components=2)
pca_proj = pca.fit_transform(features_scaled)
# Plot PCA projection
unique_species = target.unique()
colors = plt.cm.tab10(np.linspace(0, 1, len(unique_species)))
plt.figure(figsize=(10, 8))
for i, species in enumerate(unique_species):
indices = target == species
plt.scatter(pca_proj[indices, 0], pca_proj[indices, 1], label=species, color=colors[i])
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Projection (2D)')
plt.legend(title='Parent Species')
plt.show()
# PCA on scaled data
pca_scaled = PCA()
pca_scaled.fit(features_scaled)
pve_scaled = pca_scaled.explained_variance_ratio_
# PCA on unscaled data
pca_unscaled = PCA()
pca_unscaled.fit(features)
pve_unscaled = pca_unscaled.explained_variance_ratio_
# Cumulative PVE
cum_pve_scaled = np.cumsum(pve_scaled)
cum_pve_unscaled = np.cumsum(pve_unscaled)
# Plot PVE
plt.figure(figsize=(12, 6))
plt.plot(pve_scaled, label="PVE (Scaled)", marker='o')
plt.plot(cum_pve_scaled, label="Cumulative PVE (Scaled)", marker='o')
plt.plot(pve_unscaled, label="PVE (Unscaled)", marker='x')
plt.plot(cum_pve_unscaled, label="Cumulative PVE (Unscaled)", marker='x')
plt.xlabel('Principal Component')
plt.ylabel('Proportion of Variance Explained')
plt.legend()
plt.title('PVE and Cumulative PVE (Scaled vs Unscaled)')
plt.show()
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
# Load training and validation data
train_data = pd.read_csv('mol_train.csv')
val_data = pd.read_csv('mol_validation.csv')
# Separate features and targets
X_train = train_data.drop(columns=['parentspecies', 'log_pSat_Pa'])
y_train = train_data['log_pSat_Pa']
X_val = val_data.drop(columns=['parentspecies', 'log_pSat_Pa'])
y_val = val_data['log_pSat_Pa']
# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
# (i) Baseline model
model = LinearRegression()
model.fit(X_train_scaled, y_train)
baseline_rmse = np.sqrt(mean_squared_error(y_val, model.predict(X_val_scaled)))
print(f"Baseline RMSE: {baseline_rmse:.4f}")
# (ii) PCA-reduced models
rmse_values = []
for n_components in range(X_train_scaled.shape[1] + 1):
if n_components == 0:
# Dimensionality 0: Only intercept
y_pred = np.full_like(y_val, y_train.mean())
else:
pca = PCA(n_components=n_components)
X_train_pca = pca.fit_transform(X_train_scaled)
X_val_pca = pca.transform(X_val_scaled)
model = LinearRegression()
model.fit(X_train_pca, y_train)
y_pred = model.predict(X_val_pca)
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
rmse_values.append(rmse)
# Plot RMSE vs dimensionality
plt.figure(figsize=(8, 6))
plt.plot(range(len(rmse_values)), rmse_values, marker='o')
plt.xlabel('Number of PCA Components')
plt.ylabel('Validation RMSE')
plt.title('RMSE vs Dimensionality')
plt.show()
# (iii) Smallest dimensionality within 1% of the best RMSE
best_rmse = min(rmse_values)
optimal_dim = np.argmin(rmse_values)
threshold = best_rmse * 1.01
for i, rmse in enumerate(rmse_values):
if rmse <= threshold:
print(f"Smallest Dimensionality within 1% of Best RMSE: {i}")
break
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree
data = pd.read_csv("mol.csv")
data_cleaned = data.drop(columns=["parentspecies", "log_pSat_Pa"])
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_cleaned)
data_non_scaled = data_cleaned.values
def kmeans_loss(data, max_clusters=20):
losses = []
for k in range(1, max_clusters + 1):
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(data)
losses.append(kmeans.inertia_)
return losses
losses_scaled = kmeans_loss(data_scaled)
losses_non_scaled = kmeans_loss(data_non_scaled)
# Plot the losses
plt.figure(figsize=(10, 6))
plt.plot(range(1, 21), losses_scaled, label="Scaled Data", marker='o')
plt.plot(range(1, 21), losses_non_scaled, label="Non-Scaled Data", marker='x')
plt.title("K-means Loss vs. Number of Clusters")
plt.xlabel("Number of Clusters")
plt.ylabel("K-means Loss (Inertia)")
plt.legend()
plt.grid(alpha=0.3)
plt.show()
k = 5
losses = []
for _ in range(1000):
kmeans = KMeans(n_clusters=k, init="random", n_init=1, random_state=None)
kmeans.fit(data_scaled)
losses.append(kmeans.inertia_)
min_loss = np.min(losses)
max_loss = np.max(losses)
threshold = min_loss * 1.01
good_initializations = np.sum(np.array(losses) <= threshold)
prob = good_initializations/len(losses)
needed = 1/prob
plt.figure(figsize=(10, 6))
plt.hist(losses, bins=100, alpha=0.7, color='blue')
plt.title("Distribution of K-means Losses (1000 Random Initializations)")
plt.xlabel("K-means Loss (Inertia)")
plt.ylabel("Frequency")
plt.grid(alpha=0.3)
plt.show()
print(f"Minimum loss: {min_loss}")
print(f"Maximum loss: {max_loss}")
print(f"Number of good initializations: {good_initializations}")
print(f"Initilizations needed: {needed}")
linkage_methods = ['single', 'complete', 'ward']
fig, axes = plt.subplots(1, 3, figsize=(18, 6))
for i, method in enumerate(linkage_methods):
Z = linkage(data_scaled, method=method)
dendrogram(Z, ax=axes[i], truncate_mode='level', p=10)
axes[i].set_title(f"{method}")
axes[i].set_xlabel("Samples")
axes[i].set_ylabel("Distance")
plt.tight_layout()
plt.show()
methods = ['single', 'complete', 'ward']
linkages = {method: linkage(data_scaled, method=method) for method in methods}
# Flat clusterings using cut_tree
n_clusters = 5  # Specify the number of clusters for comparison
cluster_labels = {method: cut_tree(linkages[method], n_clusters=n_clusters).flatten() for method in methods}
# Compare cluster sizes for different linkage methods
cluster_sizes = {
method: np.bincount(labels) for method, labels in cluster_labels.items()
}
# Display cluster sizes for each method
print("Cluster Sizes:")
for method, sizes in cluster_sizes.items():
print(f"{method.capitalize()} Linkage: {sizes}")
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# Load data
data = pd.read_csv('mol.csv')
features = data.drop(columns=['parentspecies', 'log_pSat_Pa'])
target = data['parentspecies']
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
pca = PCA(n_components=2)
pca_proj = pca.fit_transform(features_scaled)
# Plot PCA projection
unique_species = target.unique()
colors = plt.cm.tab10(np.linspace(0, 1, len(unique_species)))
plt.figure(figsize=(10, 8))
for i, species in enumerate(unique_species):
indices = target == species
plt.scatter(pca_proj[indices, 0], pca_proj[indices, 1], label=species, color=colors[i])
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Projection (2D)')
plt.legend(title='Parent Species')
plt.show()
# PCA on scaled data
pca_scaled = PCA()
pca_scaled.fit(features_scaled)
pve_scaled = pca_scaled.explained_variance_ratio_
# PCA on unscaled data
pca_unscaled = PCA()
pca_unscaled.fit(features)
pve_unscaled = pca_unscaled.explained_variance_ratio_
# Cumulative PVE
cum_pve_scaled = np.cumsum(pve_scaled)
cum_pve_unscaled = np.cumsum(pve_unscaled)
# Plot PVE
plt.figure(figsize=(12, 6))
plt.plot(pve_scaled, label="PVE (Scaled)", marker='o')
plt.plot(cum_pve_scaled, label="Cumulative PVE (Scaled)", marker='o')
plt.plot(pve_unscaled, label="PVE (Unscaled)", marker='x')
plt.plot(cum_pve_unscaled, label="Cumulative PVE (Unscaled)", marker='x')
plt.xlabel('Principal Component')
plt.ylabel('Proportion of Variance Explained')
plt.legend()
plt.title('PVE and Cumulative PVE (Scaled vs Unscaled)')
plt.show()
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
# Load training and validation data
train_data = pd.read_csv('mol_train.csv')
val_data = pd.read_csv('mol_validation.csv')
# Separate features and targets
X_train = train_data.drop(columns=['parentspecies', 'log_pSat_Pa'])
y_train = train_data['log_pSat_Pa']
X_val = val_data.drop(columns=['parentspecies', 'log_pSat_Pa'])
y_val = val_data['log_pSat_Pa']
# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
# (i) Baseline model
model = LinearRegression()
model.fit(X_train_scaled, y_train)
baseline_rmse = np.sqrt(mean_squared_error(y_val, model.predict(X_val_scaled)))
print(f"Baseline RMSE: {baseline_rmse:.4f}")
# (ii) PCA-reduced models
rmse_values = []
for n_components in range(X_train_scaled.shape[1] + 1):
if n_components == 0:
# Dimensionality 0: Only intercept
y_pred = np.full_like(y_val, y_train.mean())
else:
pca = PCA(n_components=n_components)
X_train_pca = pca.fit_transform(X_train_scaled)
X_val_pca = pca.transform(X_val_scaled)
model = LinearRegression()
model.fit(X_train_pca, y_train)
y_pred = model.predict(X_val_pca)
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
rmse_values.append(rmse)
# Plot RMSE vs dimensionality
plt.figure(figsize=(8, 6))
plt.plot(range(len(rmse_values)), rmse_values, marker='o')
plt.xlabel('Number of PCA Components')
plt.ylabel('Validation RMSE')
plt.title('RMSE vs Dimensionality')
plt.show()
# (iii) Smallest dimensionality within 1% of the best RMSE
best_rmse = min(rmse_values)
optimal_dim = np.argmin(rmse_values)
threshold = best_rmse * 1.01
for i, rmse in enumerate(rmse_values):
if rmse <= threshold:
print(f"Smallest Dimensionality within 1% of Best RMSE: {i}")
break
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage, cut_tree
data = pd.read_csv("mol.csv")
data_cleaned = data.drop(columns=["parentspecies", "log_pSat_Pa"])
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_cleaned)
data_non_scaled = data_cleaned.values
def kmeans_loss(data, max_clusters=20):
losses = []
for k in range(1, max_clusters + 1):
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(data)
losses.append(kmeans.inertia_)
return losses
losses_scaled = kmeans_loss(data_scaled)
losses_non_scaled = kmeans_loss(data_non_scaled)
# Plot the losses
plt.figure(figsize=(10, 6))
plt.plot(range(1, 21), losses_scaled, label="Scaled Data", marker='o')
plt.plot(range(1, 21), losses_non_scaled, label="Non-Scaled Data", marker='x')
plt.title("K-means Loss vs. Number of Clusters")
plt.xlabel("Number of Clusters")
plt.ylabel("K-means Loss (Inertia)")
plt.legend()
plt.grid(alpha=0.3)
plt.show()
k = 5
losses = []
for _ in range(1000):
kmeans = KMeans(n_clusters=k, init="random", n_init=1, random_state=None)
kmeans.fit(data_scaled)
losses.append(kmeans.inertia_)
min_loss = np.min(losses)
max_loss = np.max(losses)
threshold = min_loss * 1.01
good_initializations = np.sum(np.array(losses) <= threshold)
prob = good_initializations/len(losses)
needed = 1/prob
plt.figure(figsize=(10, 6))
plt.hist(losses, bins=100, alpha=0.7, color='blue')
plt.title("Distribution of K-means Losses (1000 Random Initializations)")
plt.xlabel("K-means Loss (Inertia)")
plt.ylabel("Frequency")
plt.grid(alpha=0.3)
plt.show()
print(f"Minimum loss: {min_loss}")
print(f"Maximum loss: {max_loss}")
print(f"Number of good initializations: {good_initializations}")
print(f"Initilizations needed: {needed}")
linkage_methods = ['single', 'complete', 'ward']
fig, axes = plt.subplots(1, 3, figsize=(18, 6))
for i, method in enumerate(linkage_methods):
Z = linkage(data_scaled, method=method)
dendrogram(Z, ax=axes[i], truncate_mode='level', p=10)
axes[i].set_title(f"{method}")
axes[i].set_xlabel("Samples")
axes[i].set_ylabel("Distance")
plt.tight_layout()
plt.show()
methods = ['single', 'complete', 'ward']
linkages = {method: linkage(data_scaled, method=method) for method in methods}
# Flat clusterings using cut_tree
n_clusters = 5  # Specify the number of clusters for comparison
cluster_labels = {method: cut_tree(linkages[method], n_clusters=n_clusters).flatten() for method in methods}
# Compare cluster sizes for different linkage methods
cluster_sizes = {
method: np.bincount(labels) for method, labels in cluster_labels.items()
}
# Display cluster sizes for each method
print("Cluster Sizes:")
for method, sizes in cluster_sizes.items():
print(f"{method.capitalize()} Linkage: {sizes}")
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# Load data
data = pd.read_csv('mol.csv')
features = data.drop(columns=['parentspecies', 'log_pSat_Pa'])
target = data['parentspecies']
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)
pca = PCA(n_components=2)
pca_proj = pca.fit_transform(features_scaled)
# Plot PCA projection
unique_species = target.unique()
colors = plt.cm.tab10(np.linspace(0, 1, len(unique_species)))
plt.figure(figsize=(10, 8))
for i, species in enumerate(unique_species):
indices = target == species
plt.scatter(pca_proj[indices, 0], pca_proj[indices, 1], label=species, color=colors[i])
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA Projection (2D)')
plt.legend(title='Parent Species')
plt.show()
# PCA on scaled data
pca_scaled = PCA()
pca_scaled.fit(features_scaled)
pve_scaled = pca_scaled.explained_variance_ratio_
# PCA on unscaled data
pca_unscaled = PCA()
pca_unscaled.fit(features)
pve_unscaled = pca_unscaled.explained_variance_ratio_
# Cumulative PVE
cum_pve_scaled = np.cumsum(pve_scaled)
cum_pve_unscaled = np.cumsum(pve_unscaled)
# Plot PVE
plt.figure(figsize=(12, 6))
plt.plot(pve_scaled, label="PVE (Scaled)", marker='o')
plt.plot(cum_pve_scaled, label="Cumulative PVE (Scaled)", marker='o')
plt.plot(pve_unscaled, label="PVE (Unscaled)", marker='x')
plt.plot(cum_pve_unscaled, label="Cumulative PVE (Unscaled)", marker='x')
plt.xlabel('Principal Component')
plt.ylabel('Proportion of Variance Explained')
plt.legend()
plt.title('PVE and Cumulative PVE (Scaled vs Unscaled)')
plt.show()
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
# Load training and validation data
train_data = pd.read_csv('mol_train.csv')
val_data = pd.read_csv('mol_validation.csv')
# Separate features and targets
X_train = train_data.drop(columns=['parentspecies', 'log_pSat_Pa'])
y_train = train_data['log_pSat_Pa']
X_val = val_data.drop(columns=['parentspecies', 'log_pSat_Pa'])
y_val = val_data['log_pSat_Pa']
# Scale the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.transform(X_val)
# (i) Baseline model
model = LinearRegression()
model.fit(X_train_scaled, y_train)
baseline_rmse = np.sqrt(mean_squared_error(y_val, model.predict(X_val_scaled)))
print(f"Baseline RMSE: {baseline_rmse:.4f}")
rmse_values = []
for n_components in range(X_train_scaled.shape[1] + 1):
if n_components == 0:
y_pred = np.full_like(y_val, y_train.mean())
else:
pca = PCA(n_components=n_components)
X_train_pca = pca.fit_transform(X_train_scaled)
X_val_pca = pca.transform(X_val_scaled)
model = LinearRegression()
model.fit(X_train_pca, y_train)
y_pred = model.predict(X_val_pca)
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
rmse_values.append(rmse)
# Plot RMSE vs dimensionality
plt.figure(figsize=(8, 6))
plt.plot(range(len(rmse_values)), rmse_values, marker='o')
plt.xlabel('Number of PCA Components')
plt.ylabel('Validation RMSE')
plt.title('RMSE vs Dimensionality')
plt.show()
best_rmse = min(rmse_values)
optimal_dim = np.argmin(rmse_values)
threshold = best_rmse * 1.01
for i, rmse in enumerate(rmse_values):
if rmse <= threshold:
print(f"Smallest Dimensionality within 1% of Best RMSE: {i}")
break
