{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, ShuffleSplit\n",
    "from data_preprocess import load_and_preprocess_data\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y, test_x) = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_features = ['apin_toluene', 'decane_toluene', 'C=C-C=O in non-aromatic ring', 'toluene',\n",
    "    'NumOfN', 'nitroester', 'nitrate', 'NumOfAtoms', 'MW']\n",
    "train_x = train_x.loc[:, ~train_x.columns.isin(dropped_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           R2       MSE max_depth  max_iter  max_features  leraning_rate  \\\n",
      "55   0.746955 -2.467405         5      1500           0.5           0.05   \n",
      "87   0.746943 -2.467667        15      1500           0.5           0.05   \n",
      "108  0.746796 -2.468978        10      1500           0.5           0.05   \n",
      "166  0.746786 -2.469194      None      1500           0.5           0.05   \n",
      "15   0.746741 -2.469502        10      1500           0.5           0.05   \n",
      "..        ...       ...       ...       ...           ...            ...   \n",
      "341  0.725771 -2.673996         5       100           0.5           0.05   \n",
      "158  0.725659 -2.675150         5       100           0.5           0.05   \n",
      "39   0.724594 -2.685317      None       100           1.0           0.15   \n",
      "174  0.724102 -2.690233      None      1500           1.0           0.15   \n",
      "368  0.723222 -2.698604      None      1000           1.0           0.15   \n",
      "\n",
      "    max_leaf_nodes  min_samples_leaf  rank R2  rank MSE  \n",
      "55              31                60        1         1  \n",
      "87              20                60        2         2  \n",
      "108             20                60        3         3  \n",
      "166             20                60        4         4  \n",
      "15             100               100        5         5  \n",
      "..             ...               ...      ...       ...  \n",
      "341            100               100      496       496  \n",
      "158           None               100      497       497  \n",
      "39            None                20      498       498  \n",
      "174           None                20      499       499  \n",
      "368           None                20      500       500  \n",
      "\n",
      "[500 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('../pickles/gb_random_search_tuning_results.pickle', 'rb') as handle:\n",
    "        gb_random_search_results = pickle.load(handle)\n",
    "except:\n",
    "params = {\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'learning_rate': [0.05, 0.1, 0.15],\n",
    "    'max_iter': [100, 1000, 1500],\n",
    "    'min_samples_leaf': [20, 60, 100],\n",
    "    'max_features': [0.5, 0.75, 1.0],\n",
    "    'max_leaf_nodes': [20, 31, 100, None]\n",
    "}\n",
    "\n",
    "r_search = RandomizedSearchCV(HistGradientBoostingRegressor(), params, n_iter=500, scoring=['r2', 'neg_mean_squared_error'], verbose=3, refit=False)\n",
    "r_search.fit(train_x, train_y)\n",
    "gb_random_search_results = r_search.cv_results_\n",
    "\n",
    "gb_random_search_tuning_results = pd.DataFrame({\n",
    "    'R2': gb_random_search_results['mean_test_r2'],\n",
    "    'MSE': gb_random_search_results['mean_test_neg_mean_squared_error'],\n",
    "    'max_depth': gb_random_search_results['param_max_depth'],\n",
    "    'max_iter': gb_random_search_results['param_max_iter'],\n",
    "    'max_features': gb_random_search_results['param_max_features'],\n",
    "    'leraning_rate': gb_random_search_results['param_learning_rate'],\n",
    "    'max_leaf_nodes': gb_random_search_results['param_max_leaf_nodes'],\n",
    "    'min_samples_leaf': gb_random_search_results['param_min_samples_leaf'],\n",
    "    'rank R2': gb_random_search_results['rank_test_r2'],\n",
    "    'rank MSE': gb_random_search_results['rank_test_neg_mean_squared_error'],\n",
    "})\n",
    "print(gb_random_search_tuning_results.sort_values('rank R2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           R2       MSE max_depth  max_iter  max_features  learning_rate  \\\n",
      "336  0.748433 -2.452987      None      1200           0.5           0.03   \n",
      "451  0.748010 -2.457181      None      1000           0.4           0.04   \n",
      "21   0.747888 -2.458278      None      1400           0.4           0.01   \n",
      "603  0.747862 -2.458545      None      1000           0.4           0.05   \n",
      "346  0.747814 -2.459080      None      1400           0.5           0.03   \n",
      "..        ...       ...       ...       ...           ...            ...   \n",
      "101  0.744538 -2.490917      None      1000           0.8           0.01   \n",
      "295  0.744414 -2.492132      None      1400           0.9           0.02   \n",
      "126  0.744351 -2.492727      None      1000           0.9           0.01   \n",
      "406  0.744196 -2.494383      None      1100           0.8           0.03   \n",
      "125  0.744120 -2.495000      None      1000           0.9           0.01   \n",
      "\n",
      "     max_leaf_nodes  min_samples_leaf  rank R2  rank MSE  \n",
      "336              31                60        1         1  \n",
      "451              31                60        2         2  \n",
      "21               31                60        3         3  \n",
      "603              31                80        4         4  \n",
      "346              31                60        5         6  \n",
      "..              ...               ...      ...       ...  \n",
      "101              31                60     1046      1046  \n",
      "295              31                50     1047      1047  \n",
      "126              31                60     1048      1048  \n",
      "406              31                60     1049      1049  \n",
      "125              31                50     1050      1050  \n",
      "\n",
      "[1050 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open('../pickles/gb_grid_search_tuning_results.pickle', 'rb') as handle:\n",
    "        gb_grid_search_results = pickle.load(handle)\n",
    "except:\n",
    "    params = {\n",
    "        'max_depth': [None],\n",
    "        'learning_rate': np.linspace(0.01, 0.07, num=7),\n",
    "        'max_iter': np.arange(1000, 1500, step=100),\n",
    "        'min_samples_leaf': np.arange(50, 100, step=10),\n",
    "        'max_features': np.linspace(0.4, 0.9, 6),\n",
    "        'max_leaf_nodes': [31]\n",
    "    }\n",
    "\n",
    "    g_search = GridSearchCV(HistGradientBoostingRegressor(), params, scoring=['r2', 'neg_mean_squared_error'], verbose=3, refit=False)\n",
    "    g_search.fit(train_x, train_y)\n",
    "    gb_grid_search_results = g_search.cv_results_\n",
    "\n",
    "gb_grid_search_tuning_results = pd.DataFrame({\n",
    "    'R2': gb_grid_search_results['mean_test_r2'],\n",
    "    'MSE': gb_grid_search_results['mean_test_neg_mean_squared_error'],\n",
    "    'max_depth': gb_grid_search_results['param_max_depth'],\n",
    "    'max_iter': gb_grid_search_results['param_max_iter'],\n",
    "    'max_features': gb_grid_search_results['param_max_features'],\n",
    "    'learning_rate': gb_grid_search_results['param_learning_rate'],\n",
    "    'max_leaf_nodes': gb_grid_search_results['param_max_leaf_nodes'],\n",
    "    'min_samples_leaf': gb_grid_search_results['param_min_samples_leaf'],\n",
    "    'rank R2': gb_grid_search_results['rank_test_r2'],\n",
    "    'rank MSE': gb_grid_search_results['rank_test_neg_mean_squared_error'],\n",
    "})\n",
    "print(gb_grid_search_tuning_results.sort_values('rank R2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/gb_random_search_tuning_results.pickle', 'wb+') as handle:\n",
    "    pickle.dump(r_search.cv_results_, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/gb_grid_search_tuning_results.pickle', 'wb+') as handle:\n",
    "    pickle.dump(g_search.cv_results_, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
